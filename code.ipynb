{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_GPU is a must for fine tuning the data using below code_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __DATA__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use \" Amod/mental_health_counseling_conversations\" dataset from HuggingFace to fine tune our LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\Software\\Anaconda\\envs\\finetune\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 2.82k/2.82k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 4.79M/4.79M [00:02<00:00, 2.19MB/s]\n",
      "Generating train split: 100%|██████████| 3512/3512 [00:00<00:00, 164227.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Amod/mental_health_counseling_conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Data Processing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil use \" microsoft/Phi-3-mini-4k-instruct \" model from Hugging face to fine tune our LLM model.\n",
    "\n",
    "for this model, the recommended prompt template is \n",
    "\n",
    " <|user|> How to explain Internet for a medieval knight?<|end|> <|assistant|>\n",
    "\n",
    "\n",
    "Hence, we will format the data in the above template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to format the data\n",
    "\n",
    "def format_dataset(row):\n",
    "\n",
    "  '''\n",
    "  This function will operate row-wise\n",
    "\n",
    "  input : row of the dataframe\n",
    "  output : formatted string\n",
    "\n",
    "  '''\n",
    "\n",
    "  context = row['Context']\n",
    "  response = row['Response']\n",
    "\n",
    "  return f\"<|user|>\\n{context}<|end|>\\n<|assistant|>{response}<|end|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping the function\n",
    "df['formatted'] = df.apply(format_dataset, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['formatted'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "We will take the particular formatted column and add it to the dataset\n",
    "\n",
    "'''\n",
    "#assigning the new column to the dataset['train']\n",
    "\n",
    "dataset['train'] = dataset['train'].add_column('text', df['formatted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We are removing 'Context' and 'Response' columns from the dataset.\n",
    "\n",
    "Keeping only 'text' column, which will be used for training the model\n",
    "\n",
    "'''\n",
    "\n",
    "dataset['train']= dataset['train'].remove_columns(['Context', 'Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']['text'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __MODEL__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "We will import the model in 4 bit config\n",
    "\n",
    "'''\n",
    "\n",
    "#imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "#model\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "\n",
    "\n",
    "#setting up the BitsAndBytes config for importing the model in 4 bit format\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# importing tokenizer and the model\n",
    "\n",
    "'''GPU is mandatory for quantization. Otherwise, it will throw error. Hence, must connect to  GPU'''\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "'''\n",
    "By enabling gradient checkpointing, we are making a trade off between speed and memory.\n",
    "Due to this setting, the gradient checkpoints will be stored in memory.\n",
    "This will decrease the speed, but will levearge the GPU memory.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "As we have converted the model into quantized model for training,\n",
    "due to this, the model behaviour might be unstable.\n",
    "\n",
    "To stablize and prepare it for PEFT (Parameter Efficient Fine Tuning) training\n",
    "we need to use prepare_model_for_kbit_training() from peft library.\n",
    "\n",
    "'''\n",
    "\n",
    "# import\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model_peft = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuring the peft LoRA parameters\n",
    "\n",
    "# import\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "\n",
    "    r = 8,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules = \"all-linear\"\n",
    ")\n",
    "\n",
    "\n",
    "# applying the LoRA congfiguration to the model\n",
    "\n",
    "# import\n",
    "\n",
    "from peft import get_peft_model\n",
    "\n",
    "model_for_training = get_peft_model(model_peft, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenizing the data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tokenization function\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "  return tokenizer(examples['text'], padding = True, truncation = True)\n",
    "\n",
    "\n",
    "# Assiging a new variable to dataset['train']\n",
    "\n",
    "dt = dataset['train']\n",
    "\n",
    "\n",
    "# mapping the tokenizing function to the dataset\n",
    "\n",
    "tokenized_data = dt.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __TRAINING__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "For training, we will use Trainer() from Transformers class.\n",
    "We will configure the Trainer, using TrainingArguments() from transformers library.\n",
    "\n",
    "'''\n",
    "\n",
    "# setting an output directory path\n",
    "\n",
    "output_dir = \"./training_results\"\n",
    "\n",
    "\n",
    "# import\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    bf16 = True,\n",
    "    do_eval = False,\n",
    "    learning_rate = 5.0e-06,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=output_dir,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1,\n",
    "    max_steps = 80,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio = 0.03\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# trainer\n",
    "\n",
    "# import\n",
    "\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = Trainer(\n",
    "\n",
    "    model = model_for_training,\n",
    "    train_dataset = tokenized_data,\n",
    "    args = training_args,\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    ")\n",
    "\n",
    "'''\n",
    "\n",
    "Data collator is used for runtime data augmentation.\n",
    "DataCollatorForLanguageModeling() will augment the text data (tokenized data) during training.\n",
    "This will increase the performance of the model as it will be more robust to overfitting.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_training.config.use_cache = False\n",
    "\n",
    "# training the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Saving the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_training.save_pretrained(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pushing it to Hugging face Hub__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_training.push_to_hub(\"[YOUR HUGGING FACE REPOSITORY]\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Loading the trained model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = \"[YOUR HUGGING FACE REPOSITORY]\"\n",
    "\n",
    "\n",
    "#import\n",
    "\n",
    "from peft import PeftConfig\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model)\n",
    "\n",
    "\n",
    "# model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "# tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "\n",
    "\n",
    "# peft model\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "model = PeftModel.from_pretrained(model_for_training, peft_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
